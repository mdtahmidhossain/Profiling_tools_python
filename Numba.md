### Numba
Numba is a just-in-time (JIT) compiler for Python code, especially for numerical and array-oriented code. It allows you to speed up your Python code by compiling it to machine code, which runs much faster than Python's interpreted code. With Numba, you can take advantage of the performance gains of compiled code without having to write any C or other low-level code.

Numba supports a subset of the Python language, including NumPy arrays and numerical operations, and can be used as a drop-in replacement for parts of your Python code that need to be optimized. You simply decorate the functions you want to compile with the **numba.jit **decorator, and Numba takes care of the rest.

In addition to its JIT compiler, Numba also provides a set of libraries for GPU programming, making it easy to write GPU-accelerated code in Python.

### To install Numba
```shell
pip install numba
```
### Here's an example to show the difference in speed between Numba and pure Python
```python
import numba
import numpy as np
import time

def calculate_mean(arr):
    """
    Calculate the mean of a NumPy array.

    Parameters
    ----------
    arr : numpy.ndarray
        The input array.

    Returns
    -------
    float
        The mean of the input array.
    """
    sum = 0
    for i in range(arr.shape[0]):
        sum += arr[i]
    return sum / arr.shape[0]

@numba.jit
def calculate_mean_numba(arr):
    """
    Calculate the mean of a NumPy array using Numba.

    Parameters
    ----------
    arr : numpy.ndarray
        The input array.

    Returns
    -------
    float
        The mean of the input array.
    """
    sum = 0
    for i in range(arr.shape[0]):
        sum += arr[i]
    return sum / arr.shape[0]

def main():
    """
    Main function to compare the speed of the two mean calculation functions.
    """
    # Generate an array of random numbers
    arr = np.random.rand(1000000)

    start = time.time()
    mean = calculate_mean(arr)
    end = time.time()
    print("Mean (Pure Python):", mean)
    print("Time (Pure Python):", end - start, "seconds")

    start = time.time()
    mean = calculate_mean_numba(arr)
    end = time.time()
    print("Mean (Numba):", mean)
    print("Time (Numba):", end - start, "seconds")

if __name__ == "__main__":
    main()

```
I will start testing with **1000000**
#### Output:
    Mean (Pure Python): 0.5002930808103765
    Time (Pure Python): 0.07777571678161621 seconds
    Mean (Numba): 0.5002930808103765
    Time (Numba): 0.12022900581359863 seconds

It looks like the output you posted is the result of running the code I provided. The mean values of the arrays generated by both functions are the same, as expected.

Regarding the time, it looks like the pure Python function is faster than the Numba function in this case. This is surprising, as Numba is typically faster than pure Python for numerical computations. It's possible that the overhead of JIT compiling the Numba function is greater than the speedup from Numba optimization in this particular case.

It's also possible that the input array size is too small to see a significant speedup from Numba. You can try increasing the size of the input array to see if the difference in performance becomes more pronounced.

Now, I tried with **1000000000**.
#### Output:
    Mean (Pure Python): 0.5000109485810165
    Time (Pure Python): 81.67988801002502 seconds
    Mean (Numba): 0.5000109485810165
    Time (Numba): 2.5274059772491455 seconds

It looks like in this case, Numba is significantly faster than pure Python. The mean values generated by both functions are still the same, which is what we expect.

The **pure Python function took 81.67 seconds** to run, while the **Numba function took only 2.52 seconds** to run. This shows that Numba is able to significantly speed up the calculation of the mean.

### Type specializations
It's possible that increasing the size of the input array would lead to an even greater difference in performance between the two functions.

Numba provides type specializations, which means that you can specify the data type of the input and output of a Numba-compiled function. This can lead to performance improvements by allowing Numba to generate more optimized code.

Here's an example of how you can use type specializations in Numba:
```python
import numpy as np
from numba import jit, float64

@jit(float64(float64, float64))
def add(a, b):
    return a + b

if __name__ == '__main__':
    print(add(1.0, 2.0))

```
In this example, we use the **float64** type for both inputs and the output of the **add** function. This means that Numba will only compile the function for inputs and outputs of type **float64**. If you try to call the function with inputs of a different type, you will get an error.

The **add** function simply returns the sum of its two inputs. When you run this code, the output will be **3.0**.

Numba supports several data types for type specializations, including 
**1. int8**
**2. int16**
**3. int32**
**4. int64**
**5. uint8**
**6. uint16**
**7. uint32**
**8. uint64**
**9. float32**
**10. float64**

Here's an example that demonstrates the use of different data types in Numba:
```python
import numpy as np
from numba import jit, int32, int64, float32, float64

@jit(int32(int32, int32))
def add_int32(a, b):
    return a + b

@jit(int64(int64, int64))
def add_int64(a, b):
    return a + b

@jit(float32(float32, float32))
def add_float32(a, b):
    return a + b

@jit(float64(float64, float64))
def add_float64(a, b):
    return a + b

if __name__ == '__main__':
    print(add_int32(1, 2))
    print(add_int64(1, 2))
    print(add_float32(1.0, 2.0))
    print(add_float64(1.0, 2.0))

```
In this example, we have four functions **add_int32, add_int64, add_float32, and add_float64**, each with a different type signature. The add_int32 function takes two inputs of type int32 and returns an int32 result. The add_int64 function takes two inputs of type int64 and returns an int64 result. The add_float32 function takes two inputs of type float32 and returns a float32 result. And the add_float64 function takes two inputs of type float64 and returns a float64 result.

When you run this code, the output will be:
    3
    3
    3.0
    3.0
    
Following tells Numba to compile the **mean** function using type specializations for **float64[:]**, which means that the input of the function is an array of float64 type and there is no output.
```python
@jit((float64[:],))
def mean(array):
    return sum(array) / len(array)

```

#### In Numba, there are two modes of operation: object mode and native mode.

**Object mode** is the default mode in which Numba compiles Python code into machine code that is executed within the Python runtime environment. This mode is more flexible than native mode, as it can handle a wider range of Python constructs and data types. However, because the machine code is executed within the Python runtime, it can be slower than native mode.

**Native mode**, on the other hand, is a higher-performance mode in which Numba compiles Python code directly into machine code that is executed outside the Python runtime environment. This mode is more restrictive than object mode, as it can only handle a limited set of Python constructs and data types. However, because the machine code is executed outside the Python runtime, it can be much faster than object mode.

| Feature | Object Mode | Native Mode |
| --- | --- | --- |
| Flexibility | High | Low |
| Performance | Moderate | High |
| Supports a wider range of Python constructs and data types | Yes | No |
| Executed within the Python runtime environment | Yes | No |

### inspect_types
**inspect_types** is a Numba function that can be used to inspect the intermediate representation of a Numba compiled function. This function can provide information about the types of arguments and variables in the function, as well as the type of the returned value.

Here's an example that demonstrates the use of **inspect_types**:
```python
import numpy as np
import numba as nb

@nb.njit
def my_func(a, b):
    c = a + b
    return c

my_func_types = nb.inspect_types(my_func)
print(my_func_types)

```
Output:
    my_func (array(float64, 1d, C), array(float64, 1d, C)) -> array(float64, 1d, C)
    
In this example, inspect_types is used to inspect the intermediate representation of the my_func function. The output shows that my_func takes two 1-dimensional arrays of type float64, and returns a 1-dimensional array of type float64.

| Numba Function | Equivalent NumPy Function |
| -------------  | -------------------------- |
| `numba.math.abs` | `numpy.abs` |
| `numba.math.exp` | `numpy.exp` |
| `numba.math.log` | `numpy.log` |
| `numba.math.log10` | `numpy.log10` |
| `numba.math.sqrt` | `numpy.sqrt` |
| `numba.math.sin` | `numpy.sin` |
| `numba.math.cos` | `numpy.cos` |
| `numba.math.tan` | `numpy.tan` |
| `numba.math.arcsin` | `numpy.arcsin` |
| `numba.math.arccos` | `numpy.arccos` |
| `numba.math.arctan` | `numpy.arctan` |

### JIT Classes
```python
import numba as nb

# Define the node type using a deferred type. This allows us to define the type later after the Node class has been created.
node_type = nb.deferred_type()

# Define the specifications for the Node class. The first specification is for the next node in the linked list, which is optional. The second specification is for the value of the node.
node_spec = [
    ('next', nb.optional(node_type)),
    ('value', nb.int64)
]

# Use the jitclass decorator to compile the Node class to machine code.
@nb.jitclass(node_spec)
class Node:
    """
    Class to represent a node in a linked list.
    
    Attributes:
        next (Node, optional): The next node in the linked list.
        value (int): The value of the node.
    """
    def __init__(self, value):
        """
        Initialize a new instance of the Node class.
        
        Args:
            value (int): The value of the node.
        """
        self.value = value
        self.next = None

# Define the node type using the instance type of the Node class. This must be done after the Node class has been created.
node_type.define(Node.class_type.instance_type)

# Define the specifications for the LinkedList class. The only specification is for the head of the linked list, which is optional.
ll_spec = [
    ('head', nb.optional(Node.class_type.instance_type))
]

# Use the jitclass decorator to compile the LinkedList class to machine code.
@nb.jitclass(ll_spec)
class LinkedList:
    """
    Linked List class that consists of Nodes
    """
    def __init__(self):
        """
        Initialize an empty linked list
        """
        self.head = None
        
    def insert_node(self, value):
        """
        Insert a new Node with the given value to the front of the linked list
        
        Parameters:
        - value (int64): Value to be stored in the Node
        """
        self.head = Node(value, next=self.head)
        
    def print_linked_list(self):
        """
        Print the linked list by following the reference to the next Node
        """
        node = self.head
        while node is not None:
            print(node.value, end=' ')
            node = node.next
        print('')

# Example usage of the LinkedList class
if __name__ == '__main__':
    ll = LinkedList()
    ll.insert_node(1)
    ll.insert_node(2)
    ll.insert_node(3)
    ll.print_linked_list()


```
This code demonstrates how to create and optimize a linked list using Numba. The linked list consists of Nodes, each of which has a value attribute and a reference to the next Node, next.

The node_type variable is defined as a deferred type, which is used to specify the type of the next attribute in the node_spec list. The Node class is then compiled using Numba's jitclass decorator and node_spec list, which defines the attributes of the class.

The node_type.define(Node.class_type.instance_type) line is used to define the type of the next attribute as the type of the Node class itself. This is necessary because the next attribute is a reference to another Node object.

###Limitations in Numba
- Numba only supports a limited subset of the Python language, which can result in code that cannot be accelerated by Numba.

- Numba can only optimize code that is purely numerical in nature. If a function contains non-numerical operations, it cannot be optimized by Numba.

- Numba relies on type information for optimization, so code that uses dynamically typed variables cannot be optimized by Numba.

- Numba's Just-In-Time (JIT) compilation can add overhead to the runtime of a function. This overhead can be significant if the function is called many times.

- Numba can only accelerate functions that are called from pure Python. If a function is called from within a C library, for example, it cannot be accelerated by Numba.

- Numba is not suitable for large-scale parallel computing, as it does not currently have support for parallel processing.
