### Numba
Numba is a just-in-time (JIT) compiler for Python code, especially for numerical and array-oriented code. It allows you to speed up your Python code by compiling it to machine code, which runs much faster than Python's interpreted code. With Numba, you can take advantage of the performance gains of compiled code without having to write any C or other low-level code.

Numba supports a subset of the Python language, including NumPy arrays and numerical operations, and can be used as a drop-in replacement for parts of your Python code that need to be optimized. You simply decorate the functions you want to compile with the **numba.jit **decorator, and Numba takes care of the rest.

In addition to its JIT compiler, Numba also provides a set of libraries for GPU programming, making it easy to write GPU-accelerated code in Python.

### To install Numba
```shell
pip install numba
```
### Here's an example to show the difference in speed between Numba and pure Python
```python
import numba
import numpy as np
import time

def calculate_mean(arr):
    """
    Calculate the mean of a NumPy array.

    Parameters
    ----------
    arr : numpy.ndarray
        The input array.

    Returns
    -------
    float
        The mean of the input array.
    """
    sum = 0
    for i in range(arr.shape[0]):
        sum += arr[i]
    return sum / arr.shape[0]

@numba.jit
def calculate_mean_numba(arr):
    """
    Calculate the mean of a NumPy array using Numba.

    Parameters
    ----------
    arr : numpy.ndarray
        The input array.

    Returns
    -------
    float
        The mean of the input array.
    """
    sum = 0
    for i in range(arr.shape[0]):
        sum += arr[i]
    return sum / arr.shape[0]

def main():
    """
    Main function to compare the speed of the two mean calculation functions.
    """
    # Generate an array of random numbers
    arr = np.random.rand(1000000)

    start = time.time()
    mean = calculate_mean(arr)
    end = time.time()
    print("Mean (Pure Python):", mean)
    print("Time (Pure Python):", end - start, "seconds")

    start = time.time()
    mean = calculate_mean_numba(arr)
    end = time.time()
    print("Mean (Numba):", mean)
    print("Time (Numba):", end - start, "seconds")

if __name__ == "__main__":
    main()

```
I will start testing with **1000000**
#### Output:
    Mean (Pure Python): 0.5002930808103765
    Time (Pure Python): 0.07777571678161621 seconds
    Mean (Numba): 0.5002930808103765
    Time (Numba): 0.12022900581359863 seconds

It looks like the output you posted is the result of running the code I provided. The mean values of the arrays generated by both functions are the same, as expected.

Regarding the time, it looks like the pure Python function is faster than the Numba function in this case. This is surprising, as Numba is typically faster than pure Python for numerical computations. It's possible that the overhead of JIT compiling the Numba function is greater than the speedup from Numba optimization in this particular case.

It's also possible that the input array size is too small to see a significant speedup from Numba. You can try increasing the size of the input array to see if the difference in performance becomes more pronounced.

Now, I tried with **1000000000**.
#### Output:
    Mean (Pure Python): 0.5000109485810165
    Time (Pure Python): 81.67988801002502 seconds
    Mean (Numba): 0.5000109485810165
    Time (Numba): 2.5274059772491455 seconds

It looks like in this case, Numba is significantly faster than pure Python. The mean values generated by both functions are still the same, which is what we expect.

The **pure Python function took 81.67 seconds** to run, while the **Numba function took only 2.52 seconds** to run. This shows that Numba is able to significantly speed up the calculation of the mean.

### Type specializations
It's possible that increasing the size of the input array would lead to an even greater difference in performance between the two functions.

Numba provides type specializations, which means that you can specify the data type of the input and output of a Numba-compiled function. This can lead to performance improvements by allowing Numba to generate more optimized code.

Here's an example of how you can use type specializations in Numba:
```python
import numpy as np
from numba import jit, float64

@jit(float64(float64, float64))
def add(a, b):
    return a + b

if __name__ == '__main__':
    print(add(1.0, 2.0))

```
In this example, we use the **float64** type for both inputs and the output of the **add** function. This means that Numba will only compile the function for inputs and outputs of type **float64**. If you try to call the function with inputs of a different type, you will get an error.

The **add** function simply returns the sum of its two inputs. When you run this code, the output will be **3.0**.

Numba supports several data types for type specializations, including 
**1. int8**
**2. int16**
**3. int32**
**4. int64**
**5. uint8**
**6. uint16**
**7. uint32**
**8. uint64**
**9. float32**
**10. float64**

Here's an example that demonstrates the use of different data types in Numba:
```python
import numpy as np
from numba import jit, int32, int64, float32, float64

@jit(int32(int32, int32))
def add_int32(a, b):
    return a + b

@jit(int64(int64, int64))
def add_int64(a, b):
    return a + b

@jit(float32(float32, float32))
def add_float32(a, b):
    return a + b

@jit(float64(float64, float64))
def add_float64(a, b):
    return a + b

if __name__ == '__main__':
    print(add_int32(1, 2))
    print(add_int64(1, 2))
    print(add_float32(1.0, 2.0))
    print(add_float64(1.0, 2.0))

```
In this example, we have four functions **add_int32, add_int64, add_float32, and add_float64**, each with a different type signature. The add_int32 function takes two inputs of type int32 and returns an int32 result. The add_int64 function takes two inputs of type int64 and returns an int64 result. The add_float32 function takes two inputs of type float32 and returns a float32 result. And the add_float64 function takes two inputs of type float64 and returns a float64 result.

When you run this code, the output will be:
    3
    3
    3.0
    3.0
    
Following tells Numba to compile the **mean** function using type specializations for **float64[:]**, which means that the input of the function is an array of float64 type and there is no output.
```python
@jit((float64[:],))
def mean(array):
    return sum(array) / len(array)

```

#### In Numba, there are two modes of operation: object mode and native mode.

**Object mode** is the default mode in which Numba compiles Python code into machine code that is executed within the Python runtime environment. This mode is more flexible than native mode, as it can handle a wider range of Python constructs and data types. However, because the machine code is executed within the Python runtime, it can be slower than native mode.

**Native mode**, on the other hand, is a higher-performance mode in which Numba compiles Python code directly into machine code that is executed outside the Python runtime environment. This mode is more restrictive than object mode, as it can only handle a limited set of Python constructs and data types. However, because the machine code is executed outside the Python runtime, it can be much faster than object mode.

| Feature | Object Mode | Native Mode |
| --- | --- | --- |
| Flexibility | High | Low |
| Performance | Moderate | High |
| Supports a wider range of Python constructs and data types | Yes | No |
| Executed within the Python runtime environment | Yes | No |

### inspect_types
**inspect_types** is a Numba function that can be used to inspect the intermediate representation of a Numba compiled function. This function can provide information about the types of arguments and variables in the function, as well as the type of the returned value.

Here's an example that demonstrates the use of **inspect_types**:
```python
import numpy as np
import numba as nb

@nb.njit
def my_func(a, b):
    c = a + b
    return c

my_func_types = nb.inspect_types(my_func)
print(my_func_types)

```
Output:
    my_func (array(float64, 1d, C), array(float64, 1d, C)) -> array(float64, 1d, C)
    
In this example, inspect_types is used to inspect the intermediate representation of the my_func function. The output shows that my_func takes two 1-dimensional arrays of type float64, and returns a 1-dimensional array of type float64.
